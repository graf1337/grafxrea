1) Задание 11: ResNet блоки с skip connections 
 
Задача: реализовать ResNet архитектуру с остаточными блоками. 
 
Требования: 
Residual блок: Conv → BatchNorm → ReLU → Conv → BatchNorm + skip → ReLU 
Bottleneck блок для уменьшения размерности 
Адаптация skip connection при изменении размерности 
ResNet-50 архитектура

2) Алгоритм работы НС по блокам

1. АРХИТЕКТУРНЫЕ БЛОКИ
1.1 ResidualBlock (Базовый остаточный блок)
text
Вход → Conv2D(3x3) → BatchNorm → ReLU → Conv2D(3x3) → BatchNorm → 
       ↓ (skip connection) ← (при необходимости: Conv2D(1x1)+BatchNorm) ↑
       Add() → ReLU → Выход
Назначение: Базовая единица ResNet, решает проблему исчезающих градиентов.

1.2 BottleneckBlock (Бутылочное горлышко)
text
Вход → Conv2D(1x1) → BatchNorm → ReLU → 
     → Conv2D(3x3) → BatchNorm → ReLU → 
     → Conv2D(1x1) → BatchNorm → 
     ↓ (skip connection) ← (при необходимости: Conv2D(1x1)+BatchNorm) ↑
     Add() → ReLU → Выход
Назначение: Эффективный блок для глубоких сетей (снижает вычислительную сложность).

2. ОСНОВНАЯ АРХИТЕКТУРА ResNet50
Этап 1: Начальные слои
text
Input(32×32×3) → 
Conv2D(7×7, 64, stride=2) → 
BatchNorm → ReLU → 
MaxPooling2D(3×3, stride=2)
Этап 2: Stage 1 (64 фильтра)
text
[Вход] → BottleneckBlock ×3 (все со stride=1) → [Выход]
Этап 3: Stage 2 (128 фильтров)
text
[Вход] → BottleneckBlock(stride=2) → BottleneckBlock ×3 (stride=1) → [Выход]
Этап 4: Stage 3 (256 фильтров)
text
[Вход] → BottleneckBlock(stride=2) → BottleneckBlock ×5 (stride=1) → [Выход]
Этап 5: Stage 4 (512 фильтров)
text
[Вход] → BottleneckBlock(stride=2) → BottleneckBlock ×2 (stride=1) → [Выход]
Этап 6: Классификация
text
GlobalAveragePooling2D() → Dense(10, softmax)
3. АЛГОРИТМ ОБУЧЕНИЯ
3.1 Инициализация
Создание модели ResNet50

Компиляция с:

Оптимизатор: Adam (lr=0.001)

Функция потерь: sparse_categorical_crossentropy

Метрика: accuracy

3.2 Обучение
text
Для каждой эпохи (1..5):
    Для каждого батча (64 примера):
        1. Прямой проход (forward pass)
        2. Вычисление потерь
        3. Обратное распространение (backpropagation)
        4. Обновление весов
    5. Валидация на тестовых данных
4. ВИЗУАЛИЗАЦИОННЫЕ БЛОКИ
4.1 Визуализация архитектуры
text
plot_model() → Сохранение в 'resnet50_architecture.png'
4.2 Визуализация активаций
text
Создание модели-активаций → 
Подача случайного изображения → 
Извлечение активаций из ключевых слоев → 
Визуализация → 'resnet50_work_visualization.png'
4.3 Графики метрик обучения
text
Сбор history из обучения →
Построение графиков accuracy и loss →
Сохранение в 'training_metrics.png'
5. КЛЮЧЕВЫЕ ОСОБЕННОСТИ АЛГОРИТМА
5.1 Skip Connections (Пропущенные соединения)
text
Условие создания shortcut:
    IF (stride != 1) OR (входные_каналы != выходные_каналы):
        shortcut = Conv2D(1×1) + BatchNorm
5.2 Инициализация весов
Все сверточные слои: 'he_normal' инициализация

BatchNorm: обучаемые параметры γ и β

5.3 Режимы работы
Training=True: BatchNorm использует статистику батча

Training=False: BatchNorm использует скользящее среднее

6. ПОТОК ДАННЫХ (DATA FLOW)
text
Синтетические данные (2000 примеров) → 
→ Предобработка (нормализация) → 
→ Разделение на train/val (80%/20%) → 
→ Обучение модели → 
→ Валидация → 
→ Визуализация результатов
7. ВЫХОДНЫЕ РЕЗУЛЬТАТЫ
resnet50_architecture.png - архитектура сети

resnet50_work_visualization.png - активации слоев

training_metrics.png - графики обучения

Обученная модель с весами

8. ВЫЧИСЛИТЕЛЬНЫЕ ХАРАКТЕРИСТИКИ
Вход: 32×32×3 (уменьшено для быстрого обучения)

Параметры: ~23-25 миллионов (ориентировочно)

Батч: 64 примера

Эпохи: 10 (для демонстрации)

3) Ответ на контрольный вопрос: 11. Чем отличаются Type-I и Type-II (ultrafuzzy) нечёткие множества? 
Type-I нечёткие множества имеют функцию принадлежности μ_A(x), которая принимает одно скалярное значение из для каждого элемента x, то есть степень принадлежности точно определена числом, например μ(высокий, 170см) = 0.8. Type-II (ultrafuzzy) нечёткие множества вводят дополнительный уровень неопределённости: функция принадлежности сама становится нечёткой и задаётся вторичной функцией μ_A(x,u), где u ∈ — это возможные значения первичной принадлежности, а μ_A(x,u) показывает степень уверенности в том, что μ_A(x) = u. Это создаёт "облако" неопределённости вокруг каждого значения принадлежности, что позволяет моделировать не только нечёткость самих данных, но и неопределённость в выборе самой функции принадлежности, например вместо точного 0.8 получается интервал [0.7, 0.9] с распределением уверенности по этим значениям. В результате Type-II множества сложнее вычислительно, но лучше справляются с задачами, где эксперт не уверен в параметрах нечётких правил.
